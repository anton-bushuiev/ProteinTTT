{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "from proteinttt.models.esm2 import ESM2TTT, DEFAULT_ESM2_35M_TTT_CFG\n",
    "from proteinttt.models.esmfold import ESMFoldTTT, DEFAULT_ESMFOLD_TTT_CFG\n",
    "from proteinttt.base import TTTConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESM2\n",
    "\n",
    "Adaptation of an official [ESM2 example](https://github.com/facebookresearch/esm) to use TTT before predicting embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-18 17:28:46,040 | INFO | step: 0, accumulated_step: 0, loss: None, perplexity: None, ttt_step_time: 0.00000, score_seq_time: None, eval_step_time: 0.00000\n",
      "2025-01-18 17:28:46,455 | INFO | step: 1, accumulated_step: 16, loss: 2.72167, perplexity: None, ttt_step_time: 0.41339, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:46,849 | INFO | step: 2, accumulated_step: 32, loss: 2.27294, perplexity: None, ttt_step_time: 0.39393, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:47,244 | INFO | step: 3, accumulated_step: 48, loss: 2.41207, perplexity: None, ttt_step_time: 0.39404, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:47,638 | INFO | step: 4, accumulated_step: 64, loss: 2.37137, perplexity: None, ttt_step_time: 0.39358, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:48,032 | INFO | step: 5, accumulated_step: 80, loss: 2.38664, perplexity: None, ttt_step_time: 0.39316, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:48,425 | INFO | step: 6, accumulated_step: 96, loss: 2.23280, perplexity: None, ttt_step_time: 0.39284, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:48,817 | INFO | step: 7, accumulated_step: 112, loss: 2.13259, perplexity: None, ttt_step_time: 0.39121, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:49,211 | INFO | step: 8, accumulated_step: 128, loss: 2.22358, perplexity: None, ttt_step_time: 0.39306, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:49,605 | INFO | step: 9, accumulated_step: 144, loss: 2.00773, perplexity: None, ttt_step_time: 0.39397, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:49,999 | INFO | step: 10, accumulated_step: 160, loss: 2.51387, perplexity: None, ttt_step_time: 0.39269, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:50,392 | INFO | step: 11, accumulated_step: 176, loss: 2.17431, perplexity: None, ttt_step_time: 0.39265, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:50,784 | INFO | step: 12, accumulated_step: 192, loss: 1.68259, perplexity: None, ttt_step_time: 0.39184, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:51,178 | INFO | step: 13, accumulated_step: 208, loss: 2.22750, perplexity: None, ttt_step_time: 0.39295, score_seq_time: None, eval_step_time: 0.00003\n",
      "2025-01-18 17:28:51,572 | INFO | step: 14, accumulated_step: 224, loss: 2.58806, perplexity: None, ttt_step_time: 0.39353, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:51,966 | INFO | step: 15, accumulated_step: 240, loss: 1.93991, perplexity: None, ttt_step_time: 0.39312, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:52,360 | INFO | step: 16, accumulated_step: 256, loss: 1.74690, perplexity: None, ttt_step_time: 0.39307, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:52,752 | INFO | step: 17, accumulated_step: 272, loss: 2.13165, perplexity: None, ttt_step_time: 0.39160, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:53,144 | INFO | step: 18, accumulated_step: 288, loss: 1.89197, perplexity: None, ttt_step_time: 0.39172, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:53,538 | INFO | step: 19, accumulated_step: 304, loss: 1.72912, perplexity: None, ttt_step_time: 0.39304, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:53,931 | INFO | step: 20, accumulated_step: 320, loss: 1.81182, perplexity: None, ttt_step_time: 0.39219, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:54,324 | INFO | step: 21, accumulated_step: 336, loss: 1.83408, perplexity: None, ttt_step_time: 0.39311, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:54,718 | INFO | step: 22, accumulated_step: 352, loss: 1.61106, perplexity: None, ttt_step_time: 0.39270, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:55,111 | INFO | step: 23, accumulated_step: 368, loss: 1.77411, perplexity: None, ttt_step_time: 0.39320, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:55,506 | INFO | step: 24, accumulated_step: 384, loss: 1.50171, perplexity: None, ttt_step_time: 0.39357, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:55,898 | INFO | step: 25, accumulated_step: 400, loss: 1.21801, perplexity: None, ttt_step_time: 0.39208, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:56,293 | INFO | step: 26, accumulated_step: 416, loss: 1.21198, perplexity: None, ttt_step_time: 0.39401, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:56,683 | INFO | step: 27, accumulated_step: 432, loss: 1.26496, perplexity: None, ttt_step_time: 0.38925, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:57,076 | INFO | step: 28, accumulated_step: 448, loss: 0.75017, perplexity: None, ttt_step_time: 0.39259, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:57,471 | INFO | step: 29, accumulated_step: 464, loss: 1.06912, perplexity: None, ttt_step_time: 0.39454, score_seq_time: None, eval_step_time: 0.00002\n",
      "2025-01-18 17:28:57,866 | INFO | step: 30, accumulated_step: 480, loss: 0.72793, perplexity: None, ttt_step_time: 0.39388, score_seq_time: None, eval_step_time: 0.00002\n",
      "torch.Size([480])\n"
     ]
    }
   ],
   "source": [
    "seq = \"HRQALGERLYPRVQAMQPAFASKITGMLLELSPAQLLLLLASEDSLRARVDEAMELII\"\n",
    "\n",
    "# Load ESM-2 model and data\n",
    "model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval().to(device)  # disables dropout for deterministic results\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter([(None, seq)])\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "# ================ TTT ================\n",
    "ttt_cfg = DEFAULT_ESM2_35M_TTT_CFG\n",
    "model = ESM2TTT.ttt_from_pretrained(model, ttt_cfg)\n",
    "model.ttt(batch_tokens)\n",
    "# =====================================\n",
    "\n",
    "# Extract per-residue representations\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[12])\n",
    "token_representations = results[\"representations\"][12]\n",
    "sequence_representations = []\n",
    "for i, tokens_len in enumerate(batch_lens):\n",
    "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "print(sequence_representations[0].shape)\n",
    "\n",
    "# Rest model to original state (after this model.ttt can be called again on another protein)\n",
    "# ================ TTT ================\n",
    "model.ttt_reset()\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESMFold\n",
    "\n",
    "Adaptation of an official [ESMFold example](https://github.com/facebookresearch/esm) to use TTT before predicting protein structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-18 18:11:08,062 | INFO | step: 0, accumulated_step: 0, loss: None, perplexity: None, ttt_step_time: 0.00000, score_seq_time: None, eval_step_time: 2.23341, plddt: 38.43025\n",
      "2025-01-18 18:11:10,325 | INFO | step: 1, accumulated_step: 4, loss: 2.50000, perplexity: None, ttt_step_time: 0.51810, score_seq_time: None, eval_step_time: 1.74307, plddt: 34.06020\n",
      "2025-01-18 18:11:12,543 | INFO | step: 2, accumulated_step: 8, loss: 2.66797, perplexity: None, ttt_step_time: 0.47896, score_seq_time: None, eval_step_time: 1.73836, plddt: 33.57555\n",
      "2025-01-18 18:11:15,073 | INFO | step: 3, accumulated_step: 12, loss: 2.46094, perplexity: None, ttt_step_time: 0.47758, score_seq_time: None, eval_step_time: 1.73828, plddt: 76.67573\n",
      "2025-01-18 18:11:17,298 | INFO | step: 4, accumulated_step: 16, loss: 2.37500, perplexity: None, ttt_step_time: 0.48372, score_seq_time: None, eval_step_time: 1.73908, plddt: 72.56865\n",
      "2025-01-18 18:11:19,959 | INFO | step: 5, accumulated_step: 20, loss: 2.37891, perplexity: None, ttt_step_time: 0.48225, score_seq_time: None, eval_step_time: 1.73907, plddt: 78.69619\n",
      "2025-01-18 18:11:22,183 | INFO | step: 6, accumulated_step: 24, loss: 2.12305, perplexity: None, ttt_step_time: 0.48552, score_seq_time: None, eval_step_time: 1.73748, plddt: 76.08324\n",
      "2025-01-18 18:11:24,402 | INFO | step: 7, accumulated_step: 28, loss: 1.87500, perplexity: None, ttt_step_time: 0.48136, score_seq_time: None, eval_step_time: 1.73749, plddt: 77.02941\n",
      "2025-01-18 18:11:26,621 | INFO | step: 8, accumulated_step: 32, loss: 1.73535, perplexity: None, ttt_step_time: 0.48025, score_seq_time: None, eval_step_time: 1.73759, plddt: 43.66622\n",
      "2025-01-18 18:11:28,839 | INFO | step: 9, accumulated_step: 36, loss: 1.15527, perplexity: None, ttt_step_time: 0.48014, score_seq_time: None, eval_step_time: 1.73738, plddt: 73.76467\n",
      "2025-01-18 18:11:31,059 | INFO | step: 10, accumulated_step: 40, loss: 1.04688, perplexity: None, ttt_step_time: 0.48120, score_seq_time: None, eval_step_time: 1.73770, plddt: 38.99184\n",
      "78.69618870728084\n"
     ]
    }
   ],
   "source": [
    "model = esm.pretrained.esmfold_v1()\n",
    "model = model.eval().cuda()\n",
    "\n",
    "# Optionally, uncomment to set a chunk size for axial attention. This can help reduce memory.\n",
    "# Lower sizes will have lower memory requirements at the cost of increased speed.\n",
    "# model.set_chunk_size(128)\n",
    "\n",
    "sequence = \"GIHLGELGLLPSTVLAIGYFENLVNIICESLNMLPKLEVSGKEYKKFKFTIVIPKDLDANIKKRAKIYFKQKSLIEIEIPTSSRNYPIHIQFDENSTDDILHLYDMPTTIGGIDKAIEMFMRKGHIGKTDQQKLLEERELRNFKTTLENLIATDAFAKEMVEVIIEE\"\n",
    "\n",
    "# ================ TTT ================\n",
    "ttt_cfg = DEFAULT_ESMFOLD_TTT_CFG\n",
    "ttt_cfg.seed = 0  # Trying TTT with several different seeds may enable finding structure with higher pLDDT\n",
    "ttt_cfg.steps = 10\n",
    "model = ESMFoldTTT.ttt_from_pretrained(model, esmfold_config=model.cfg)\n",
    "model.ttt(sequence)\n",
    "# =====================================\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.infer_pdb(sequence)\n",
    "\n",
    "with open(\"result.pdb\", \"w\") as f:\n",
    "    f.write(output)\n",
    "\n",
    "import biotite.structure.io as bsio\n",
    "struct = bsio.load_structure(\"result.pdb\", extra_fields=[\"b_factor\"])\n",
    "print(struct.b_factor.mean())  # this will be the pLDDT\n",
    "\n",
    "# Rest model to original state (after this model.ttt can be called again on another protein)\n",
    "# ================ TTT ================\n",
    "model.ttt_reset()\n",
    "# ====================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
